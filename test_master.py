# test_master.py - FIXED VERSION
# A V√âGS≈ê, KOMPLETT, M≈∞K√ñD≈êK√âPES TESZT-PROTOKOLL

import asyncio
import argparse
import random
import re
from typing import List, Dict, Any
from datetime import datetime

# A rendszer minden sz√ºks√©ges komponens√©nek import√°l√°sa
from database.db import get_db_session
from database.models import Article
from scraper.news_scraper import NewsScraper
from ai.editorial_ai import StrategicEditorialAI
from ai.v5_orchestrator import ChimeraDualChannelOrchestrator
from config.sources import NEWS_SOURCES

# === TESZT KONFIGUR√ÅCI√ì ===
QUICK_MODE_SOURCES = [
    # MAGYAR PR√âMIUM
    'Telex', 'HVG', 'Index', '24.hu', 'V√°lasz Online', 'G7', 'Portfolio',
    
    # MAGYAR STANDARD  
    '444.hu', 'Magyar Nemzet', 'Maszol',
    
    # BBC CSAL√ÅD
    'BBC News UK', 'BBC News World', 'BBC Business', 'BBC Technology',
    
    # GUARDIAN CSAL√ÅD  
    'The Guardian World', 'The Guardian UK', 'The Guardian Technology',
    
    # CNN CSAL√ÅD
    'CNN Latest', 'CNN World', 'CNN Technology',
    
    # GAZDAS√ÅGI PR√âMIUM
    'The Economist - Finance', 'The Economist - Business', 'Bloomberg Markets',
    
    # TECH PR√âMIUM
    'TechCrunch', 'TechCrunch Startups', 'The Verge', 'WIRED Business',
    
    # OKNYOMOZ√ì PR√âMIUM
    'Bellingcat', 'The Intercept', 'ProPublica', 'OCCRP',
    
    # TOV√ÅBBI NEMZETK√ñZI
    'Fox News', 'Politico EU', 'AP News Top Stories', 'Sky News Home',
    
    # N√âMET FORR√ÅSOK
    'FAZ', 'Tagesspiegel', 'NZZ'
]
ARTICLE_FETCH_LIMIT_QUICK = 5
ARTICLE_FETCH_LIMIT_COMPLETE = 10
# =================================

def print_pipeline_report(editorial_results: Dict, final_dispositions: List[Dict]):
    """A cs≈ëvezet√©k elemz√©s√©nek riportja (cikk√≠r√°s n√©lk√ºl)."""
    scraped_count = editorial_results.get("scraped_count", 0)
    kept_count = len(editorial_results.get("kept", []))
    dup_count = len(editorial_results.get("duplicates", []))

    print("\n" + "="*130)
    print(f"{'üìä CS≈êVEZET√âK-ELEMZ√âSI JELENT√âS üìä':^130}")
    print("="*130)

    print("\n--- 1-2. F√ÅZIS: H√çRSZERZ√âS √âS SZERKESZT≈êI SZ≈∞R√âS ---")
    print(f"Begy≈±jt√∂tt cikkek: {scraped_count} db | AI √°ltal egyedinek √≠t√©lt: {kept_count} db | Kisz≈±rt duplik√°tum: {dup_count} db")

    if not final_dispositions:
        print("\nNincs tov√°bb feldolgozott cikk a riportol√°shoz.")
        print("="*130)
        return

    print("\n--- 3-4. F√ÅZIS: ORCHESTRATOR - CSATORNAKIOSZT√ÅS √âS PARANCSKIAD√ÅS (TELJES LISTA) ---")
    header = f"| {'#':<3} | {'Forr√°s':<20} | {'C√≠m (R√©szlet)':<45} | {'AI Kateg√≥ria':<12} | {'AI Pont':<7} | {'Csatorna':<10} | {'√öjs√°g√≠r√≥':<22} |"
    print(header)
    print(f"|{'-'*5}|{'-'*22}|{'-'*47}|{'-'*14}|{'-'*9}|{'-'*12}|{'-'*24}|")

    for i, disp in enumerate(final_dispositions):
        title_short = (disp.get('title', 'N/A')[:43] + '..') if len(disp.get('title', 'N/A')) > 45 else disp.get('title', 'N/A')
        row = (f"| {i+1:<3} | {disp.get('source', 'N/A'):<20} | {title_short:<45} | "
               f"{disp.get('category', 'N/A'):<12} | {disp.get('importance', 'N/A'):<7} | "
               f"{disp.get('channel', 'N/A'):<10} | {disp.get('journalist', 'N/A'):<22} |")
        print(row)
    
    print("="*130)

def print_generation_report(processed_articles: List[Article]):
    """A legener√°lt cikkek riportja."""
    print("\n" + "="*125)
    print(f"{'‚úíÔ∏è √âLESL√ñV√âSZET - ELK√âSZ√úLT CIKKEK JELENT√âSE ‚úíÔ∏è':^125}")
    print("="*125)
    
    if not processed_articles:
        print("A feldolgoz√°s sor√°n nem gener√°l√≥dott √∫j cikk.")
        print("="*125)
        return
        
    for i, art in enumerate(processed_articles):
        print(f"\n\n--- CIKK #{i+1} ---")
        ai_title = art.ai_title or "GENER√ÅLT C√çM HI√ÅNYZIK"
        signature = f"{art.journalist_name}, H√≠rMagnet" if art.journalist_name else "H√≠rMagnet Szerkeszt≈ës√©g"
        summary = art.ai_summary or "A TARTALOM GENER√ÅL√ÅSA SIKERTELEN VAGY HI√ÅNYOS."
        
        print(f"Forr√°s: {art.source}")
        print(f"Eredeti C√≠m: {art.original_title}")
        print("-" * 25)
        print(f"GENER√ÅLT C√çM: {ai_title}")
        print(f"SZIGN√ì: {signature}")
        print(f"AI Kateg√≥ria: {art.category} | AI Pontsz√°m: {int(art.importance_score or 0)}/20")
        print("-" * 25)
        print("\nGENER√ÅLT TARTALOM:\n")
        print(summary)
        print("\n" + "="*50 + f" CIKK #{i+1} V√âGE " + "="*51)

    print("\n\n‚úÖ Inspekci√≥ befejezve.")

def _get_dispositions_from_channels(orchestrator, channels):
    """Seg√©df√ºggv√©ny a riportol√°shoz sz√ºks√©ges adatok kinyer√©s√©re."""
    final_dispositions = []
    
    # Blitz channel feldolgoz√°sa
    blitz_articles = channels.get('blitz', [])
    for article in blitz_articles:
        # √öjs√°g√≠r√≥ kiv√°laszt√°s (ha van journalist manager)
        journalist_assignment = None
        journalist_name = "N/A"
        
        try:
            if hasattr(orchestrator, 'processor') and orchestrator.processor.journalist_manager:
                journalist_assignment = orchestrator.processor.journalist_manager.select_journalist_for_article(
                    getattr(article, 'category', 'general'),
                    getattr(article, 'importance_score', 8),
                    getattr(article, 'original_content', '') or ''
                )
                if journalist_assignment:
                    journalist_name = journalist_assignment.get('journalist_name', 'N/A')
        except:
            pass
        
        final_dispositions.append({
            'title': getattr(article, 'original_title', 'N/A'),
            'source': getattr(article, 'source', 'N/A'),
            'category': getattr(article, 'category', 'N/A'),
            'importance': getattr(article, 'importance_score', 'N/A'),
            'channel': 'Blitz',
            'journalist': journalist_name
        })
    
    # Strategic channel feldolgoz√°sa
    strategic_articles = channels.get('strategic', [])
    for article in strategic_articles:
        # √öjs√°g√≠r√≥ kiv√°laszt√°s (ha van journalist manager)
        journalist_assignment = None
        journalist_name = "N/A"
        
        try:
            if hasattr(orchestrator, 'processor') and orchestrator.processor.journalist_manager:
                journalist_assignment = orchestrator.processor.journalist_manager.select_journalist_for_article(
                    getattr(article, 'category', 'general'),
                    getattr(article, 'importance_score', 8),
                    getattr(article, 'original_content', '') or ''
                )
                if journalist_assignment:
                    journalist_name = journalist_assignment.get('journalist_name', 'N/A')
        except:
            pass
        
        final_dispositions.append({
            'title': getattr(article, 'original_title', 'N/A'),
            'source': getattr(article, 'source', 'N/A'),
            'category': getattr(article, 'category', 'N/A'),
            'importance': getattr(article, 'importance_score', 'N/A'),
            'channel': 'Strategic',
            'journalist': journalist_name
        })
    
    return final_dispositions

async def run_master_test(mode: str, force_scrape: bool, generate_content: bool):
    """Az univerz√°lis teszt-szkript f≈ë logik√°ja."""
    print(f"üéØ Operation 'Meisterst√ºck' indul... M√≥dusz: '{mode.upper()}' | Force Scrape: {force_scrape} | Generate Content: {generate_content}")
    
    # DEBUG: API kulcsok ellen≈ërz√©se
    print("üîß API kulcsok ellen≈ërz√©se...")
    import os
    openai_key = os.getenv('OPENAI_API_KEY')
    gemini_key = os.getenv('GEMINI_API_KEY')
    print(f"üîß OPENAI_API_KEY: {'‚úÖ SET' if openai_key else '‚ùå MISSING'}")
    print(f"üîß GEMINI_API_KEY: {'‚úÖ SET' if gemini_key else '‚ùå MISSING'}")
    
    db = get_db_session()
    
    # 1. L√âP√âS: DINAMIKUS H√çRGY≈∞JT√âS
    if mode == 'quick':
        target_sources_configs = [s for s in NEWS_SOURCES if s['name'] in QUICK_MODE_SOURCES and s.get('active', True)]
        fetch_limit = ARTICLE_FETCH_LIMIT_QUICK
    else:
        target_sources_configs = [s for s in NEWS_SOURCES if s.get('active', True)]
        fetch_limit = ARTICLE_FETCH_LIMIT_COMPLETE
        
    source_names = [s['name'] for s in target_sources_configs]
    print(f"üîç H√≠rszerz√©s indul... {len(source_names)} forr√°s a c√©lkeresztben.")
    
    # === A HI√ÅNYZ√ì LOGIKA BEILLESZTVE ===
    scraper = NewsScraper()
    scraped_articles: List[Article] = []
    
    NewsScraper._scrape_single_source_for_entries = _scrape_single_source_for_entries
    NewsScraper._create_article_object_from_entry = _create_article_object_from_entry
    NewsScraper._clean_html_content = lambda self, text: re.sub(r'<[^>]+>', '', text or "")

    urls_seen = set() if force_scrape else {row.url for row in db.query(Article.url)}
    all_entries = []

    for source_config in target_sources_configs:
        feed = scraper._scrape_single_source_for_entries(source_config)
        for entry in feed.entries[:fetch_limit]:
            all_entries.append({'entry': entry, 'source_config': source_config})

    all_entries.sort(key=lambda x: x['entry'].get('published_parsed', datetime.min.timetuple()), reverse=True)

    for item in all_entries:
        entry = item['entry']
        article_url = entry.get('link')
        if not article_url or article_url in urls_seen:
            continue
        urls_seen.add(article_url)
        article_obj = scraper._create_article_object_from_entry(entry, item['source_config'])
        if len(article_obj.original_content or "") > 150:
             scraped_articles.append(article_obj)
    # =================================================

    if not scraped_articles:
        print("\n‚ùå NEM TAL√ÅLHAT√ì FELDOLGOZHAT√ì CIKK A MEGADOTT KRIT√âRIUMOKKAL. A teszt le√°ll.")
        db.close()
        return

    print(f"‚úÖ H√≠rszerz√©s k√©sz: {len(scraped_articles)} cikk begy≈±jtve.")
    
    # 2. L√âP√âS: EL≈êZETES SZ≈∞R√âS (csak ha nincs gener√°l√°s)
    if not scraped_articles:
        print("‚úÖ Nincs feldolgozhat√≥ cikk.")
        db.close()
        return
        
    # 3. L√âP√âS: D√ñNT√âS A FOLYTAT√ÅSR√ìL
    orchestrator = ChimeraDualChannelOrchestrator()
    if not generate_content:
        print("ü§ñ Cikk-gener√°l√°s KIHAGYVA. Cs≈ëvezet√©k-elemz√©s riportol√°sa...")
        
        # CSAK pipeline elemz√©shez h√≠vjuk meg az Editorial AI-t
        editorial_ai = StrategicEditorialAI()
        editorial_results = editorial_ai.process_articles_editorial(scraped_articles)
        articles_to_process = editorial_results.get("kept", [])
        
        if not articles_to_process:
            print("‚úÖ Az Editorial AI minden cikket kisz≈±rt.")
            db.close()
            return
            
        channels = orchestrator.categorize_articles_by_channel(articles_to_process)
        final_dispositions = _get_dispositions_from_channels(orchestrator, channels)
        editorial_results["scraped_count"] = len(scraped_articles)
        print_pipeline_report(editorial_results, final_dispositions)
        print("‚úÖ Cs≈ëvezet√©k teszt befejezve.")
    else:
        print("‚úçÔ∏è Cikk-gener√°l√°s INDUL...")
        
        # NINCS Editorial AI h√≠v√°s itt - az orchestrator v√©gzi!
        # UPSERT Logika - k√∂zvetlen√ºl a scraped articles-ekkel
        articles_for_orchestrator = []
        for mem_article in scraped_articles:
            db_article = db.query(Article).filter(Article.url == mem_article.url).first()
            if db_article:
                # Friss√≠tj√ºk a l√©tez≈ë cikket
                db_article.original_title = mem_article.original_title
                db_article.original_content = mem_article.original_content
                db_article.is_processed = False
                articles_for_orchestrator.append(db_article)
            else:
                db.add(mem_article)
                articles_for_orchestrator.append(mem_article)
        db.commit()
        
        # Az orchestrator elv√©gzi az Editorial AI feldolgoz√°st + gener√°l√°st
        article_ids_to_track = [art.id for art in articles_for_orchestrator]
        print(f"üîß Database √≠r√°s - {len(articles_for_orchestrator)} cikk az orchestrator-nak √°tadva...")
        await orchestrator.run_full_process(articles_for_orchestrator)
        
        print("‚úÖ Cikk-gener√°l√°si f√°zis befejezve. Eredm√©nyek lek√©rdez√©se...")
        processed_articles = db.query(Article).filter(Article.id.in_(article_ids_to_track)).all()
        processed_count = len([art for art in processed_articles if art.ai_title or art.ai_summary])
        print(f"üîß √öj cikkek sz√°ma: {processed_count}")
        print_generation_report(processed_articles)
    
    db.close()

# Seg√©df√ºggv√©nyek, hogy a szkript √∂n√°ll√≥an futtathat√≥ legyen
def _scrape_single_source_for_entries(self, source):
    import feedparser
    try:
        response = self.session.get(source['url'], timeout=15)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except Exception as e:
        return type('MockFeed', (), {'entries': []})()

def _create_article_object_from_entry(self, entry, source_config):
    from dateutil import parser as date_parser
    title = entry.get('title', '').strip()
    content = entry.summary if 'summary' in entry else entry.description if 'description' in entry else ""
    clean_content = self._clean_html_content(content)
    published_at = None
    if 'published_parsed' in entry and entry.published_parsed:
        published_at = datetime(*entry.published_parsed[:6])
    elif 'published' in entry:
        try: published_at = date_parser.parse(entry.published) 
        except: pass
    return Article(
        title=title, original_title=title, original_content=clean_content,
        url=entry.get('link'), source=source_config['name'], 
        category=source_config.get('category', 'general'), published_at=published_at
    )

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='H√≠rMagnet Univerz√°lis Teszt Protokoll v2.1 - FIXED')
    parser.add_argument('--mode', type=str, choices=['quick', 'complete'], default='quick', help="A teszt m√≥dusza.")
    parser.add_argument('--force-scrape', action='store_true', help="Figyelmen k√≠v√ºl hagyja a megl√©v≈ë cikkeket.")
    parser.add_argument('--generate-content', action='store_true', help="Lefuttatja a t√©nyleges cikk-gener√°l√°st is.")
    args = parser.parse_args()
    
    asyncio.run(run_master_test(args.mode, args.force_scrape, args.generate_content))
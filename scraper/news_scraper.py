# news_scraper.py - OPERATION STAHLFILTER & STRATEGISCHE PRIORIT√ÑT UPGRADE
# German Engineering Precision by Oberleutnant von Program

import feedparser
import requests
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from database.db import get_db_session
from database.models import Article, ProcessingLog
from config.sources import NEWS_SOURCES
from config.settings import MAX_ARTICLES_PER_SOURCE, REQUEST_TIMEOUT
import time
import hashlib
from dateutil import parser as date_parser

# === TAKTIKAI DEKONTAMIN√ÅCI√ìS EGYS√âG IMPORT√ÅL√ÅSA ===
import re
from bs4 import BeautifulSoup
# ====================================================


# === JAV√çT√ÅS: STAHLFILTER KONFIGUR√ÅCI√ì (SZ√ì-ALAP√ö) ===
MINIMUM_WORD_COUNT = 100  # Az Ac√©lsz≈±r≈ë minim√°lis sz√≥limtje
# =======================================================

class NewsScraperError(Exception):
    pass

class NewsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
        })
        
    def scrape_all_sources(self):
        """√ñsszes akt√≠v forr√°s lescrapel√©se STRAT√âGIAI PRIORIT√ÅS szerint."""
        total_new_articles = 0
        db = get_db_session()
        
        start_time = time.time()
        
        try:
            # A forr√°sok sorba rendez√©se a `priority` mez≈ë alapj√°n
            sorted_sources = sorted([s for s in NEWS_SOURCES if s.get('active', True)], key=lambda x: x.get('priority', 99))
            print(f"üéñÔ∏è Strat√©giai sorrend fel√°ll√≠tva. {len(sorted_sources)} akt√≠v forr√°s a c√©lkeresztben.")

            for source in sorted_sources:
                print(f"\nüîç T√°mad√°s alatt: {source['name']} (Priorit√°s: {source.get('priority', 'N/A')})")
                try:
                    new_count = self._scrape_single_source(source, db)
                    total_new_articles += new_count
                    if new_count > 0:
                        print(f"‚úÖ Sikeres behatol√°s: {source['name']} - {new_count} √∫j cikk biztos√≠tva.")
                    else:
                        print(f"‚ìò {source['name']}: Nincs √∫j harc√°szati √©rt√©k≈± inform√°ci√≥.")
                    
                    time.sleep(0.5)
                    
                except Exception as e:
                    error_message = f"Hiba a(z) '{source['name']}' frontszakasz√°n: {str(e)}"
                    print(f"‚ùå {error_message}")
                    self._log_error(db, "scraping", source['name'], error_message)
                    continue
            
            processing_time = time.time() - start_time
            
            log = ProcessingLog(
                action="scraping_v2_strategic",
                articles_processed=total_new_articles,
                success=True,
                processing_time=processing_time
            )
            db.add(log)
            db.commit()
            
            print(f"\nüéâ SCRAPING HADM≈∞VELET BEFEJEZVE! √ñsszesen {total_new_articles} √∫j cikk biztos√≠tva {processing_time:.1f} m√°sodperc alatt.")
            return total_new_articles
            
        except Exception as e:
            self._log_error(db, "scraping", "general", str(e))
            raise NewsScraperError(f"√Åltal√°nos scraping hiba: {str(e)}")
        finally:
            db.close()

    def _clean_html_content(self, html_content: str) -> str:
        """Taktikai Dekontamin√°ci√≥s elj√°r√°s a nyers HTML tartalomhoz."""
        if not html_content:
            return ""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text()
            return re.sub(r'\s+', ' ', text).strip()
        except Exception:
            text = re.sub(r'<[^>]+>', '', html_content)
            return re.sub(r'\s+', ' ', text).strip()

    def _scrape_single_source(self, source, db: Session):
        """Egy forr√°s lescrapel√©se"""
        try:
            response = self.session.get(source['url'], timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            if not feed.entries: return 0
            
            new_articles = 0
            for entry in feed.entries[:MAX_ARTICLES_PER_SOURCE]:
                try:
                    if self._save_article(entry, source, db):
                        new_articles += 1
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Cikk ment√©si hiba ({entry.get('link', 'N/A')}): {str(e)}")
                    continue
            return new_articles
        except Exception as e:
            raise Exception(f"RSS let√∂lt√©si hiba")
    
    def _save_article(self, entry, source_config, db: Session):
        """Cikk ment√©se az adatb√°zisba a meger≈ës√≠tett STAHLFILTER doktr√≠n√°val."""
        try:
            article_url = entry.get('link', '')
            if not article_url: return False
                
            existing = db.query(Article).filter(Article.url == article_url).first()
            if existing: return False
            
            title = entry.get('title', '').strip()
            if not title: return False
                
            published_at = None
            if 'published' in entry:
                try:
                    published_at = date_parser.parse(entry.published)
                except:
                    pass
            
            raw_content = ""
            if 'summary' in entry:
                raw_content = entry.summary
            elif 'description' in entry:
                raw_content = entry.description

            clean_content = self._clean_html_content(raw_content)

            # === JAV√çT√ÅS: STAHLFILTER DOKTR√çNA (SZ√ì-ALAP√ö) ===
            # A karakterek helyett a szavak sz√°m√°t ellen≈ërizz√ºk
            word_count = len(clean_content.split())
            if word_count < MINIMUM_WORD_COUNT:
                #print(f"  üõ°Ô∏è STAHLFILTER: Cikk elutas√≠tva - t√∫l r√∂vid ({word_count} sz√≥). C√≠m: {title[:50]}...")
                return False
            # ===============================================
            
            article = Article(
                title=title,
                original_title=title,
                original_content=clean_content,
                url=article_url,
                source=source_config['name'],
                category=source_config.get('category', 'general'),
                published_at=published_at,
                is_processed=False
            )
            
            db.add(article)
            db.commit()
            
            return True
            
        except Exception as e:
            db.rollback()
            raise Exception(f"Adatb√°zis ment√©si hiba: {str(e)}")
    
    def _log_error(self, db: Session, action: str, source: str, error: str):
        """Hiba napl√≥z√°sa"""
        try:
            log = ProcessingLog(action=action, source=source, success=False, error_message=error[:1000])
            db.add(log)
            db.commit()
        except:
            pass

def main():
    """F≈ë scraping f√ºggv√©ny (cron job-hoz)"""
    scraper = NewsScraper()
    try:
        new_articles = scraper.scrape_all_sources()
        print(f"‚úÖ Scraping k√©sz: {new_articles} √∫j cikk")
        return new_articles
    except Exception as e:
        print(f"‚ùå KRITIKUS SCRAPING HIBA: {str(e)}")
        return 0

if __name__ == "__main__":
    main()

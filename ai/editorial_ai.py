# ai/editorial_ai.py - v5.0 - OPERATION EFFIZIENZ
# Egyes√≠tett, intelligens el≈ëfeldolgoz√°s

import google.generativeai as genai
from typing import List, Dict, Optional, Any
import time
import re
import json
import os
from difflib import SequenceMatcher

# A rendszer t√∂bbi r√©sz√©vel val√≥ kompatibilit√°s meg≈ërz√©se
from database.models import Article
try:
    from ai.prompt_manager import get_prompt_manager
    PROMPT_MANAGER_AVAILABLE = True
except ImportError:
    PROMPT_MANAGER_AVAILABLE = False


class StrategicEditorialAI:
    """
    üß≤ H√≠rMagnet Strategic Editorial AI v5.0 - EFFIZIENZ
    
    NEUE FEATURES in v5.0:
    ‚úÖ Egyes√≠tett Felder√≠t√©si Protokoll: Kategoriz√°l√°s, fontoss√°g-becsl√©s √©s duplik√°tum-ujjlenyomat egyetlen AI h√≠v√°ssal.
    ‚úÖ A "buta" kulcsszavas sz≈±r≈ë teljes kiiktat√°sa.
    ‚úÖ Dr√°maian cs√∂kkentett hibasz√°zal√©k √©s megn√∂velt hat√©konys√°g.
    ‚úÖ Gemini 2.5 Flash az intelligens elemz√©shez.
    ‚úÖ Finomhangolt 0.55 similarity threshold az optim√°lis duplik√°tum-detekt√°l√°shoz.
    """
    
    def __init__(self):
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if gemini_api_key:
            genai.configure(api_key=gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')
            print("‚úÖ Strategic Editorial AI v5.0 (EFFIZIENZ) inicializ√°lva Gemini 2.5 Flash-sel")
        else:
            self.gemini_model = None
            print("‚ö†Ô∏è GEMINI_API_KEY hi√°nyzik! Editorial AI letiltva.")
        
        # A PromptManager itt m√°r csak fallback c√©lokat szolg√°lhat, a f≈ë prompt bels≈ë.
        self.prompt_manager = get_prompt_manager() if PROMPT_MANAGER_AVAILABLE else None

    def _get_unified_analysis_prompt(self, article: Article) -> str:
        """
        L√©trehozza az √∫j, EGYES√çTETT FELDER√çT√âSI √âS BESOROL√ÅSI PARANCS v1.1-et.
        """
        clean_content = self._clean_content(article.original_content or "")
        return f"""
        Elemezd a k√∂vetkez≈ë magyar h√≠rcikket a megadott szempontok szerint. A v√°laszodat kiz√°r√≥lag egyetlen, valid JSON objektumk√©nt add vissza.

        CIKK C√çME: {article.original_title}
        CIKK TARTALMA (R√âSZLET): {clean_content[:2500]}

        FELADATOK:
        1.  **Kategoriz√°l√°s:** Hat√°rozd meg a cikk legpontosabb kateg√≥ri√°j√°t a k√∂vetkez≈ë, z√°rt list√°b√≥l: [politics, economy, tech, sport, entertainment, foreign, lifestyle, cars, general]. A d√∂nt√©sed a cikk val√≥s tartalm√°n alapuljon.
        2.  **Fontoss√°g Becsl√©se:** Adj egy el≈ëzetes fontoss√°gi pontsz√°mot 1-20 k√∂z√∂tti sk√°l√°n, ahol a 20 a vil√°gszinten is kiemelt h√≠r.
        3.  **Duplik√°tum Ujjlenyomat:** Hozz l√©tre egy t√©ma-ujjlenyomatot a cikkr≈ël a k√©s≈ëbbi duplik√°tum-sz≈±r√©shez.
            - **KRITIKUS PARANCS:** Az ujjlenyomatot a C√çM **√âS** a TARTALOM egy√ºttes elemz√©se alapj√°n kell l√©trehozni a maxim√°lis pontoss√°g √©rdek√©ben!
            - `main_topic`: A cikk f≈ë t√©m√°ja 2-4 sz√≥ban (pl. "korm√°nyinf√≥ bejelent√©s", "√∫j iPhone modell").
            - `key_entities`: A cikkben szerepl≈ë legfontosabb szem√©lyek, helyek, szervezetek (maximum 4).

        V√ÅLASZ KIZ√ÅR√ìLAG JSON FORM√ÅTUMBAN:
        {{
            "real_category": "meghat√°rozott_kateg√≥ria",
            "importance_score": "<1-20 k√∂z√∂tti eg√©sz sz√°m>",
            "duplicate_fingerprint": {{
                "main_topic": "f≈ë t√©ma 2-4 sz√≥ban",
                "key_entities": ["entit√°s_1", "entit√°s_2"]
            }},
            "reasoning": "R√∂vid, 1-2 mondatos indokl√°s a kateg√≥ria √©s fontoss√°g v√°laszt√°s√°r√≥l."
        }}
        """

    def _get_unified_analysis(self, article: Article) -> Optional[Dict]:
        """Egyetlen AI h√≠v√°ssal lefuttatja a kategoriz√°l√°st, fontoss√°g-becsl√©st √©s ujjlenyomat-k√©sz√≠t√©st."""
        if not self.gemini_model:
            return None
        
        prompt = self._get_unified_analysis_prompt(article)
        try:
            response = self.gemini_model.generate_content(prompt)
            json_text_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_text_match:
                return json.loads(json_text_match.group(0))
            print(f"      ‚ö†Ô∏è AI Unified Analysis - JSON nem tal√°lhat√≥ a v√°laszban: {response.text}")
            return None
        except Exception as e:
            print(f"      ‚ö†Ô∏è AI Unified Analysis hiba: {str(e)}")
            return None

    def process_articles_editorial(self, articles: List[Article]) -> Dict[str, Any]:
        """A teljes, √∫j, intelligens el≈ëfeldolgoz√°si cs≈ëvezet√©k."""
        if not self.gemini_model:
            return {"kept": articles, "duplicates": [], "merged": [], "analysis": {}}

        print(f"\nüìù STRATEGIC EDITORIAL AI v5.0 PROCESSING (EFFIZIENZ PROTOKOLL)")
        print(f"üìä Input: {len(articles)} cikk")
        
        # 1. F√ÅZIS: EGYES√çTETT AI ELEMZ√âS MINDEN CIKKRE
        print("ü§ñ Phase 1: Egyes√≠tett AI elemz√©s futtat√°sa...")
        start_time = time.time()
        
        for article in articles:
            analysis = self._get_unified_analysis(article)
            # Az eredm√©nyt ideiglenesen az objektumhoz csatoljuk
            article.ai_analysis = analysis if analysis else {
                "real_category": article.category, "importance_score": 8,
                "duplicate_fingerprint": {"main_topic": article.original_title[:30], "key_entities": []},
                "reasoning": "AI analysis fallback"
            }
            # A kateg√≥ri√°t √©s a pontsz√°mot azonnal friss√≠tj√ºk az objektumon
            article.category = article.ai_analysis.get("real_category", article.category)
            article.importance_score = article.ai_analysis.get("importance_score", 8)
            
            print(f"   üìä {article.source}: {article.category} (score: {article.importance_score})")
            time.sleep(0.3) # Rate limiting

        print(f"   ‚úÖ Elemz√©s k√©sz: {time.time() - start_time:.1f}s")

        # 2. F√ÅZIS: DUPLIK√ÅTUM-SZ≈∞R√âS AZ AI UJJLENYOMATOK ALAPJ√ÅN
        print("üîç Phase 2: Duplik√°tum-sz≈±r√©s az intelligens ujjlenyomatok alapj√°n...")
        kept_articles: List[Article] = []
        duplicates: List[Article] = []
        # A 'processed_topics' mostant√≥l a m√°r megtartott cikkek list√°ja lesz
        
        # El≈ësz√∂r rendezz√ºk a cikkeket a kapott fontoss√°g szerint, hogy a fontosabb maradjon meg
        sorted_articles = sorted(articles, key=lambda a: a.importance_score, reverse=True)

        for article in sorted_articles:
            is_duplicate = False
            current_fingerprint = article.ai_analysis.get("duplicate_fingerprint", {})
            
            for kept_article in kept_articles:
                # Hasonl√≥s√°gi pontsz√°m sz√°m√≠t√°sa a k√©t ujjlenyomat k√∂z√∂tt
                kept_fingerprint = kept_article.ai_analysis.get("duplicate_fingerprint", {})
                similarity = self._calculate_fingerprint_similarity(current_fingerprint, kept_fingerprint)
                
                # FINOMHANGOLT THRESHOLD: 0.55 (volt: 0.75)
                if similarity > 0.55:
                    is_duplicate = True
                    print(f"   üóëÔ∏è Duplik√°tum (sim: {similarity:.2f}): [{article.source}] '{article.original_title[:40]}...' ~ [{kept_article.source}] '{kept_article.original_title[:40]}...'. Elvetve.")
                    duplicates.append(article)
                    break

            if not is_duplicate:
                kept_articles.append(article)
                print(f"   ‚úÖ Megtartva: [{article.source}] '{article.original_title[:40]}...' (score: {article.importance_score})")
        
        print(f"\n   üìä Sz≈±r√©s k√©sz:")
        print(f"      ‚úÖ Megtartva: {len(kept_articles)} cikk")
        print(f"      üóëÔ∏è Duplik√°tumok: {len(duplicates)} cikk")
        print(f"      üìà Duplik√°tum ar√°ny: {(len(duplicates)/len(articles)*100):.1f}%")
        
        return {
            "kept": kept_articles,
            "duplicates": duplicates,
            "merged": [],
            "analysis": {
                "message": "Efficiency Protocol Complete", 
                "duplicate_rate": len(duplicates)/len(articles) if articles else 0,
                "processing_time": time.time() - start_time
            }
        }

    def _calculate_fingerprint_similarity(self, fp1: Dict, fp2: Dict) -> float:
        """Kisz√°molja a hasonl√≥s√°got k√©t AI √°ltal gener√°lt ujjlenyomat k√∂z√∂tt."""
        if not fp1 or not fp2: 
            return 0.0

        topic1 = fp1.get("main_topic", "").lower()
        topic2 = fp2.get("main_topic", "").lower()
        topic_sim = SequenceMatcher(None, topic1, topic2).ratio()

        entities1 = set([e.lower() for e in fp1.get("key_entities", [])])
        entities2 = set([e.lower() for e in fp2.get("key_entities", [])])
        
        if not entities1 and not entities2: 
            entity_overlap = 0.0
        elif not entities1 or not entities2: 
            entity_overlap = 0.0
        else:
            intersection = len(entities1.intersection(entities2))
            union = len(entities1.union(entities2))
            entity_overlap = intersection / union if union > 0 else 0

        # Weighted combination: 60% topic + 40% entities
        final_similarity = (topic_sim * 0.6) + (entity_overlap * 0.4)
        
        return final_similarity

    def _clean_content(self, content: str) -> str:
        """HTML c√≠mk√©k elt√°vol√≠t√°sa √©s egyszer≈±s√≠t√©s."""
        if not content: 
            return ""
        # HTML tags removal + whitespace cleanup
        clean = re.sub(r'<[^>]+>', '', content)
        clean = re.sub(r'\s+', ' ', clean).strip()
        return clean

    def get_session_statistics(self) -> Dict:
        """Enhanced session statistics for v5.0"""
        return {
            "version": "5.0_EFFIZIENZ",
            "gemini_model": "2.5-flash-preview",
            "similarity_threshold": 0.55,
            "features": [
                "unified_analysis", 
                "intelligent_categorization", 
                "ai_fingerprint_deduplication",
                "importance_scoring"
            ],
            "gemini_available": self.gemini_model is not None,
            "prompt_manager_available": self.prompt_manager is not None
        }

# Wrapper class a visszamen≈ëleges kompatibilit√°s√©rt
class EnhancedEditorialAI(StrategicEditorialAI):
    """
    EnhancedEditorialAI - Wrapper a StrategicEditorialAI-hoz
    √Åtir√°ny√≠tja a funkcionalit√°st a StrategicEditorialAI-ra.
    """
    def __init__(self):
        super().__init__()
        print("‚úÖ EnhancedEditorialAI inicializ√°lva (StrategicEditorialAI v5.0 wrapper)")
    
    def process_articles_editorial(self, articles: List[Article]) -> Dict:
        """
        √Åtir√°ny√≠tja a feldolgoz√°st a StrategicEditorialAI-ra
        """
        return super().process_articles_editorial(articles)

def main():
    """Strategic Editorial AI v5.0 testing"""
    print("üß™ Strategic Editorial AI v5.0 EFFIZIENZ teszt...")
    
    editorial = StrategicEditorialAI()
    
    # Basic functionality test
    stats = editorial.get_session_statistics()
    print(f"‚úÖ Version: {stats['version']}")
    print(f"‚úÖ Gemini Model: {stats['gemini_model']}")
    print(f"‚úÖ Similarity Threshold: {stats['similarity_threshold']}")
    print(f"‚úÖ Features: {', '.join(stats['features'])}")
    
    print("üéØ Strategic Editorial AI v5.0 EFFIZIENZ ready for deployment!")

if __name__ == "__main__":
    main()
